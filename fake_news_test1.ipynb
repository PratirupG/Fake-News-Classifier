{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fake News Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pa \n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.layers import LSTM,Embedding,Bidirectional,Flatten,Dense,GlobalAveragePooling1D\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras import optimizers\n",
    "from keras.preprocessing.text import one_hot,Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping\n",
    "from nltk.tokenize import word_tokenize,TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import KeyedVectors\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sn \n",
    "import warnings \n",
    "import string\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To-Do Text Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Spacy Text processing EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "stopwords = stopwords.words('english')\n",
    "punctuation = string.punctuation\n",
    "engstopwords = set(stopwords)\n",
    "APPO = {\n",
    "\"aren't\" : \"are not\",\n",
    "\"can't\" : \"cannot\",\n",
    "\"couldn't\" : \"could not\",\n",
    "\"didn't\" : \"did not\",\n",
    "\"doesn't\" : \"does not\",\n",
    "\"don't\" : \"do not\",\n",
    "\"hadn't\" : \"had not\",\n",
    "\"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"I would\",\n",
    "\"i'd\" : \"I had\",\n",
    "\"i'll\" : \"I will\",\n",
    "\"i'm\" : \"I am\",\n",
    "\"isn't\" : \"is not\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\",\n",
    "\"i've\" : \"I have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"mightn't\" : \"might not\",\n",
    "\"mustn't\" : \"must not\",\n",
    "\"shan't\" : \"shall not\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"shouldn't\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"we'll\" : \"we will\"   ,\n",
    "\"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who're\" : \"who are\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"won't\" : \"will not\",\n",
    "\"wouldn't\" : \"would not\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'll\":\" will\",\n",
    "\"didn't\": \"did not\",\n",
    "\"tryin'\":\"trying\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean the data\n",
    "class Helper_Class:\n",
    "    def __init__(self):\n",
    "        print('Helper Class Called')\n",
    "        self.all_unique_words = None\n",
    "   \n",
    "    def clean_data(self,data):\n",
    "        try:\n",
    "            re_email = re.compile(r'[\\w.-]+@[\\w.-]+')\n",
    "            data= re_email.sub(r'',data)\n",
    "            reg_website = re.compile(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([\\w+-a-zA-Z0-9()@:%_\\+.~#?&//=]*)')\n",
    "            data= reg_website.sub(r'',data)\n",
    "            data = data.lower()\n",
    "            words = tokenizer.tokenize(data)\n",
    "            words = [APPO[word] if word in APPO else word for word in words]\n",
    "            clean_data = [word for word in words if word not in punctuation]\n",
    "            cleaned_data =  \" \".join(clean_data)\n",
    "            return cleaned_data\n",
    "        except Exception:\n",
    "            print('Problem in document')\n",
    "            print(data)\n",
    "    \n",
    "    def unique_words(self,data_req):\n",
    "        ##Total Unique words\n",
    "        vocab = data_req['description'].values\n",
    "        all_words = []\n",
    "\n",
    "        for sent in vocab:\n",
    "            words = tokenizer.tokenize(sent)\n",
    "            for word in words:\n",
    "                all_words.append(word)\n",
    "        \n",
    "        unique_word = set(all_words)\n",
    "        self.all_unique_words = unique_word\n",
    "\n",
    "        print('Total Words {}'.format(len(all_words)))\n",
    "        print('Unique Words {}'.format(len(unique_word)))\n",
    "\n",
    "        vocab_length = len(unique_word)+50\n",
    "        return vocab_length\n",
    "\n",
    "    def largest_sentence(self,data_req):\n",
    "        ##Largest Sentence\n",
    "        length = lambda Sentence: len(tokenizer.tokenize(Sentence))\n",
    "        max_legth = max(data_req['description'],key=length)\n",
    "        max_legth_sent_len = len(tokenizer.tokenize(max_legth))\n",
    "        return max_legth_sent_len\n",
    "\n",
    "    def Embedded_Padded_Sent(self,data_req,vocab_length,max_legth):\n",
    "        padded = self.__embedded_sent(data_req,vocab_length,max_legth)\n",
    "        return padded\n",
    "\n",
    "    def __embedded_sent(self,data_req,vocab_length,max_length):\n",
    "        #calcualte embedded sentence\n",
    "        embedded_sentence = [one_hot(sent,vocab_length) for sent in data_req['description']]\n",
    "        padded_sentence = self.__padded_sent(embedded_sentence,max_length)\n",
    "        return padded_sentence\n",
    "\n",
    "    def __padded_sent(self,embedded_sentence,max_length):\n",
    "        #Calculate padded_sent\n",
    "        return  pad_sequences(embedded_sentence,max_length,padding='post')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ALL_MODELS\n",
    "class Models:\n",
    "    def __init__(self,X_data,Y_data,unique_words):\n",
    "        self.__X = X_data\n",
    "        self.__Y = Y_data\n",
    "        self.__all__unique_words = unique_words \n",
    "\n",
    "    def Custom_Embedding(self,vocab_length,longest_length):\n",
    "        models =Sequential()\n",
    "        models.add(Embedding(vocab_length,20,input_length=longest_length))\n",
    "        models.add(Flatten())\n",
    "        models.add(Dense(1,activation='sigmoid'))\n",
    "        models.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])\n",
    "        print(models.summary())\n",
    "        models.fit(self.__X, self.__Y, epochs=20, verbose=0)\n",
    "        loss, accuracy = models.evaluate(self.__X, self.__Y, verbose=0)\n",
    "        print('Accuracy: %f' % (accuracy*100))\n",
    "        return models\n",
    "\n",
    "    def LSTM_CUSTOM_EMB(self,vocab_length,longest_length):\n",
    "        models = Sequential()\n",
    "        models.add(Embedding(vocab_length,20,input_length=longest_length))\n",
    "        models.add(LSTM(10,activation='relu'))\n",
    "        models.add(Dense(1,activation='sigmoid'))\n",
    "        models.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])\n",
    "        print(models.summary())\n",
    "        models.fit(self.__X, self.__Y, epochs=2, verbose=0)\n",
    "        accuracy = models.evaluate(self.__X,self.__Y,verbose=0)\n",
    "        print('Accuracy: %f' % (accuracy*100))\n",
    "        return models\n",
    "\n",
    "    def word2vec_LSTM(self,location,vocab_length,longest_length):\n",
    "        word_embedding_mat = self.Word2VEc_Embeddings(location,vocab_length)\n",
    "        \n",
    "        #Build Model model\n",
    "        models = Sequential()\n",
    "        models.add(Embedding(vocab_length,100,weights=[word_embedding_mat],input_length=longest_length,\n",
    "        trainable=False))\n",
    "        models.add(LSTM(10,activation='relu',return_sequences=True))\n",
    "        models.add(GlobalAveragePooling1D())\n",
    "        models.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "        models.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])\n",
    "        print(models.summary())\n",
    "\n",
    "        models.fit(self.__X, self.__Y, epochs=20, verbose=0)\n",
    "        loss, accuracy = models.evaluate(self.__X, self.__Y, verbose=0)\n",
    "        print('Accuracy %f'% (accuracy*100))\n",
    "\n",
    "        return models\n",
    "\n",
    "    ##Keras functional API\n",
    "    def HYBRID_LSTM_WORD2VEC(self,location,vocab_length,longest_length):\n",
    "        ## Todo\n",
    "        pass\n",
    "\n",
    "    def Word2VEc_Embeddings(self,location,vocab_length):\n",
    "        word_embedding_mat = self.__read_word_embedding(location,vocab_length)\n",
    "        return word_embedding_mat\n",
    "\n",
    "    def __read_word_embedding(self,file_location,vocab_length):\n",
    "        ##Give the address to read all the word embedding matruix\n",
    "        location = file_location\n",
    "        word_embeddings = self.__create_embedded_matrix(location,vocab_length)\n",
    "        return word_embeddings\n",
    "\n",
    "    def __create_embedded_matrix(self,location,vocab_length):\n",
    "        embedding_index = KeyedVectors.load_word2vec_format(location,binary=True)\n",
    "        embedding_matrix = (np.random.rand(vocab_length,100) - 0.5)/5.0\n",
    "\n",
    "        for i,word in enumerate(self.__all__unique_words):\n",
    "            if word in embedding_index:\n",
    "                embedding_vector = embedding_index.get_vector(word)\n",
    "                embedding_matrix[i] = embedding_vector[:100]\n",
    "        \n",
    "        return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "17880\n17879\nValues {0: 17014, 1: 865}\n"
    }
   ],
   "source": [
    "data = pa.read_csv('fake_job_postings.csv')\n",
    "data_req = data[['description','fraudulent']]\n",
    "print(len(data_req))\n",
    "#Value count target for checking if it's imbalanced\n",
    "data_req=data_req.dropna(axis=0)\n",
    "print(len(data_req))\n",
    "values = dict(data_req['fraudulent'].value_counts())\n",
    "print('Values {}'.format(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Helper Class Called\nTotal Words 3133278\nUnique Words 90553\nMax Length sentence 2166\nPadded_Sentence index 1 [71250 44266   152 ...     0     0     0]\n"
    }
   ],
   "source": [
    "helper_obj = Helper_Class()\n",
    "data_req['description']  =  data_req['description'].apply(lambda x: helper_obj.clean_data(x))\n",
    "vocab_length = helper_obj.unique_words(data_req)\n",
    "max_legth_sent_len = helper_obj.largest_sentence(data_req)\n",
    "padded_sentence = helper_obj.Embedded_Padded_Sent(data_req,vocab_length,max_legth_sent_len)\n",
    "\n",
    "print('Max Length sentence {}'.format(max_legth_sent_len))\n",
    "print('Padded_Sentence index 1 {}'.format(padded_sentence[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Helper Class Called\nTotal Words 3133278\nUnique Words 90552\nMax Length sentence 2166\nPadded_Sentence index 1 [ 4129 31209 35427 ...     0     0     0]\n"
    }
   ],
   "source": [
    "helper_obj = Helper_Class()\n",
    "data_req['description']  =  data_req['description'].apply(lambda x: helper_obj.clean_data(x))\n",
    "vocab_length = helper_obj.unique_words(data_req)\n",
    "max_legth_sent_len = helper_obj.largest_sentence(data_req)\n",
    "padded_sentence = helper_obj.Embedded_Padded_Sent(data_req,vocab_length,max_legth_sent_len)\n",
    "\n",
    "print('Max Length sentence {}'.format(max_legth_sent_len))\n",
    "print('Padded_Sentence index 1 {}'.format(padded_sentence[0]))\n",
    "\n",
    "##Divide the data\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(padded_sentence,data_req['fraudulent'],test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_11\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_11 (Embedding)     (None, 2166, 20)          1812040   \n_________________________________________________________________\nflatten_9 (Flatten)          (None, 43320)             0         \n_________________________________________________________________\ndense_10 (Dense)             (None, 1)                 43321     \n=================================================================\nTotal params: 1,855,361\nTrainable params: 1,855,361\nNon-trainable params: 0\n_________________________________________________________________\nNone\nAccuracy: 99.993008\nTest Accuracy 0.9812639951705933\n"
    }
   ],
   "source": [
    "##Build Model\n",
    "unique_words = helper_obj.all_unique_words\n",
    "embedd_mod = Models(X_train,Y_train,unique_words)\n",
    "embedd_model=embedd_mod.Custom_Embedding(vocab_length,max_legth_sent_len)\n",
    "loss,accuracy = embedd_model.evaluate(X_test,Y_test,verbose=0)\n",
    "print('Test Accuracy {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedd_mod = Models(padded_sentence,data_req['fraudulent'],unique_words)\n",
    "location = r'D:\\GoogleNews-vectors-negative300.bin'\n",
    "model_word2 = embedd_mod.word2vec_LSTM(location,vocab_length,max_legth_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Actual class 0\nPredicted class 0\n"
    }
   ],
   "source": [
    "##Prediction\n",
    "predicted = embedd_model.predict_classes(np.reshape(X_test[0],newshape=(1,2166)))\n",
    "print('Actual class {}'.format(Y_test.values[0]))\n",
    "print('Predicted class {}'.format(predicted[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bittfgpu2conda6869594dd3c349e79f9a3c302d5f44f6",
   "display_name": "Python 3.7.7 64-bit ('tf_gpu_2': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}